{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff3d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec95dd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Act',\n",
       " 'Alhamdullah',\n",
       " 'all',\n",
       " 'Baba',\n",
       " 'Basis',\n",
       " 'because',\n",
       " 'Boy',\n",
       " 'Brave',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'College',\n",
       " 'Confused',\n",
       " 'conversation',\n",
       " 'Crying',\n",
       " 'daily',\n",
       " 'danger',\n",
       " 'disagree with',\n",
       " 'drinks',\n",
       " 'Eats',\n",
       " 'effort',\n",
       " 'Egypt',\n",
       " 'Enters',\n",
       " 'Excelent',\n",
       " 'explanation',\n",
       " 'Fasting',\n",
       " 'Female',\n",
       " 'First',\n",
       " 'For Us',\n",
       " 'fuel',\n",
       " 'Gift',\n",
       " 'Girl',\n",
       " 'Glass',\n",
       " 'good bye',\n",
       " 'government',\n",
       " 'Happy',\n",
       " 'hates',\n",
       " 'hears',\n",
       " 'Help',\n",
       " 'Here You Are',\n",
       " 'how_are_u',\n",
       " 'Humanity',\n",
       " 'hungry',\n",
       " 'I',\n",
       " 'ignorance',\n",
       " 'immediately',\n",
       " 'Important',\n",
       " 'Intelligent',\n",
       " 'Last',\n",
       " 'leader',\n",
       " 'Liar',\n",
       " 'Loves',\n",
       " 'male',\n",
       " 'Mama',\n",
       " 'memory',\n",
       " 'model',\n",
       " 'mostly',\n",
       " 'motive',\n",
       " 'Muslim',\n",
       " 'must',\n",
       " 'my home land',\n",
       " 'no',\n",
       " 'nonsense',\n",
       " 'obvious',\n",
       " 'Old',\n",
       " 'Palestine',\n",
       " 'prevent',\n",
       " 'ready',\n",
       " 'rejection',\n",
       " 'Right',\n",
       " 'selects',\n",
       " 'shut up',\n",
       " 'Sing',\n",
       " 'sleeps',\n",
       " 'smells',\n",
       " 'Somkes',\n",
       " 'Spoon',\n",
       " 'Summer',\n",
       " 'symposium',\n",
       " 'Tea',\n",
       " 'teacher',\n",
       " 'terrifying',\n",
       " 'Thanks',\n",
       " 'time',\n",
       " 'to boycott',\n",
       " 'to cheer',\n",
       " 'to go',\n",
       " 'to live',\n",
       " 'to spread',\n",
       " 'Toilet',\n",
       " 'trap',\n",
       " 'University',\n",
       " 'ur_welcome',\n",
       " 'victory',\n",
       " 'walks',\n",
       " 'Where',\n",
       " 'wheres_ur_house',\n",
       " 'Window',\n",
       " 'Winter',\n",
       " 'yes',\n",
       " 'You']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'C:/Users/terbo/Desktop/final_data_npy'\n",
    "actions = []\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    actions.append(dir_)\n",
    "actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a3f799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Act': 0,\n",
       " 'Alhamdullah': 1,\n",
       " 'all': 2,\n",
       " 'Baba': 3,\n",
       " 'Basis': 4,\n",
       " 'because': 5,\n",
       " 'Boy': 6,\n",
       " 'Brave': 7,\n",
       " 'calls': 8,\n",
       " 'calm': 9,\n",
       " 'College': 10,\n",
       " 'Confused': 11,\n",
       " 'conversation': 12,\n",
       " 'Crying': 13,\n",
       " 'daily': 14,\n",
       " 'danger': 15,\n",
       " 'disagree with': 16,\n",
       " 'drinks': 17,\n",
       " 'Eats': 18,\n",
       " 'effort': 19,\n",
       " 'Egypt': 20,\n",
       " 'Enters': 21,\n",
       " 'Excelent': 22,\n",
       " 'explanation': 23,\n",
       " 'Fasting': 24,\n",
       " 'Female': 25,\n",
       " 'First': 26,\n",
       " 'For Us': 27,\n",
       " 'fuel': 28,\n",
       " 'Gift': 29,\n",
       " 'Girl': 30,\n",
       " 'Glass': 31,\n",
       " 'good bye': 32,\n",
       " 'government': 33,\n",
       " 'Happy': 34,\n",
       " 'hates': 35,\n",
       " 'hears': 36,\n",
       " 'Help': 37,\n",
       " 'Here You Are': 38,\n",
       " 'how_are_u': 39,\n",
       " 'Humanity': 40,\n",
       " 'hungry': 41,\n",
       " 'I': 42,\n",
       " 'ignorance': 43,\n",
       " 'immediately': 44,\n",
       " 'Important': 45,\n",
       " 'Intelligent': 46,\n",
       " 'Last': 47,\n",
       " 'leader': 48,\n",
       " 'Liar': 49,\n",
       " 'Loves': 50,\n",
       " 'male': 51,\n",
       " 'Mama': 52,\n",
       " 'memory': 53,\n",
       " 'model': 54,\n",
       " 'mostly': 55,\n",
       " 'motive': 56,\n",
       " 'Muslim': 57,\n",
       " 'must': 58,\n",
       " 'my home land': 59,\n",
       " 'no': 60,\n",
       " 'nonsense': 61,\n",
       " 'obvious': 62,\n",
       " 'Old': 63,\n",
       " 'Palestine': 64,\n",
       " 'prevent': 65,\n",
       " 'ready': 66,\n",
       " 'rejection': 67,\n",
       " 'Right': 68,\n",
       " 'selects': 69,\n",
       " 'shut up': 70,\n",
       " 'Sing': 71,\n",
       " 'sleeps': 72,\n",
       " 'smells': 73,\n",
       " 'Somkes': 74,\n",
       " 'Spoon': 75,\n",
       " 'Summer': 76,\n",
       " 'symposium': 77,\n",
       " 'Tea': 78,\n",
       " 'teacher': 79,\n",
       " 'terrifying': 80,\n",
       " 'Thanks': 81,\n",
       " 'time': 82,\n",
       " 'to boycott': 83,\n",
       " 'to cheer': 84,\n",
       " 'to go': 85,\n",
       " 'to live': 86,\n",
       " 'to spread': 87,\n",
       " 'Toilet': 88,\n",
       " 'trap': 89,\n",
       " 'University': 90,\n",
       " 'ur_welcome': 91,\n",
       " 'victory': 92,\n",
       " 'walks': 93,\n",
       " 'Where': 94,\n",
       " 'wheres_ur_house': 95,\n",
       " 'Window': 96,\n",
       " 'Winter': 97,\n",
       " 'yes': 98,\n",
       " 'You': 99}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0927bc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_labels = np.array(actions)\n",
    "temp_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb3c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_DIR, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(1,30,2):\n",
    "            res = np.load(os.path.join(DATA_DIR, action, str(sequence), \"{}.npy\".format(frame_num))) \n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6c234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26100, 15, 84)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6de886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26100, 100)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.utils as tf_utils\n",
    "y = tf_utils.to_categorical(labels).astype(int)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e430d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86031d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class StopOnAccuracy(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, target_acc):\n",
    "        self.target_acc = target_acc\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['categorical_accuracy'] >= self.target_acc:\n",
    "            print(\"\\nReached target accuracy, stopping training...\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc129203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\terbo\\anaconda3\\envs\\sign_language_env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - categorical_accuracy: 0.0748 - loss: 3.6131\n",
      "Epoch 2/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - categorical_accuracy: 0.2142 - loss: 2.5482\n",
      "Epoch 3/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.3124 - loss: 2.1466\n",
      "Epoch 4/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.4109 - loss: 1.8371\n",
      "Epoch 5/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.5207 - loss: 1.4757\n",
      "Epoch 6/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.6030 - loss: 1.2122\n",
      "Epoch 7/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.6618 - loss: 1.0489\n",
      "Epoch 8/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.7120 - loss: 0.9010\n",
      "Epoch 9/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.7493 - loss: 0.7954\n",
      "Epoch 10/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.7747 - loss: 0.7206\n",
      "Epoch 11/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.7981 - loss: 0.6452\n",
      "Epoch 12/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 22ms/step - categorical_accuracy: 0.8155 - loss: 0.6074\n",
      "Epoch 13/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.8410 - loss: 0.5356\n",
      "Epoch 14/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.8447 - loss: 0.5165\n",
      "Epoch 15/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.8561 - loss: 0.4821\n",
      "Epoch 16/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.8636 - loss: 0.4673\n",
      "Epoch 17/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.8773 - loss: 0.4174\n",
      "Epoch 18/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.8905 - loss: 0.3920\n",
      "Epoch 19/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.8896 - loss: 0.3827\n",
      "Epoch 20/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9020 - loss: 0.3575\n",
      "Epoch 21/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9060 - loss: 0.3397\n",
      "Epoch 22/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9144 - loss: 0.3174\n",
      "Epoch 23/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9148 - loss: 0.3084\n",
      "Epoch 24/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9179 - loss: 0.3023\n",
      "Epoch 25/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9198 - loss: 0.2938\n",
      "Epoch 26/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9136 - loss: 0.3154\n",
      "Epoch 27/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9336 - loss: 0.2545\n",
      "Epoch 28/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9311 - loss: 0.2557\n",
      "Epoch 29/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9360 - loss: 0.2424\n",
      "Epoch 30/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9372 - loss: 0.2510\n",
      "Epoch 31/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9396 - loss: 0.2316\n",
      "Epoch 32/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9376 - loss: 0.2397\n",
      "Epoch 33/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9451 - loss: 0.2107\n",
      "Epoch 34/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9430 - loss: 0.2301\n",
      "Epoch 35/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9509 - loss: 0.1941\n",
      "Epoch 36/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9437 - loss: 0.2194\n",
      "Epoch 37/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9469 - loss: 0.1999\n",
      "Epoch 38/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9528 - loss: 0.1892\n",
      "Epoch 39/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9523 - loss: 0.1928\n",
      "Epoch 40/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9552 - loss: 0.1818\n",
      "Epoch 41/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9532 - loss: 0.1850\n",
      "Epoch 42/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9537 - loss: 0.1853\n",
      "Epoch 43/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9528 - loss: 0.1811\n",
      "Epoch 44/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9594 - loss: 0.1704\n",
      "Epoch 45/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9607 - loss: 0.1616\n",
      "Epoch 46/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9516 - loss: 0.1948\n",
      "Epoch 47/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9606 - loss: 0.1571\n",
      "Epoch 48/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9666 - loss: 0.1407\n",
      "Epoch 49/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9610 - loss: 0.1559\n",
      "Epoch 50/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9626 - loss: 0.1535\n",
      "Epoch 51/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9624 - loss: 0.1541\n",
      "Epoch 52/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9649 - loss: 0.1420\n",
      "Epoch 53/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9644 - loss: 0.1477\n",
      "Epoch 54/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9685 - loss: 0.1385\n",
      "Epoch 55/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9636 - loss: 0.1553\n",
      "Epoch 56/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9669 - loss: 0.1352\n",
      "Epoch 57/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9682 - loss: 0.1335\n",
      "Epoch 58/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9702 - loss: 0.1240\n",
      "Epoch 59/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9687 - loss: 0.1273\n",
      "Epoch 60/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - categorical_accuracy: 0.9693 - loss: 0.1296\n",
      "Epoch 61/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - categorical_accuracy: 0.9693 - loss: 0.1276\n",
      "Epoch 62/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 22ms/step - categorical_accuracy: 0.9734 - loss: 0.1119\n",
      "Epoch 63/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9697 - loss: 0.1308\n",
      "Epoch 64/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9727 - loss: 0.1183\n",
      "Epoch 65/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9713 - loss: 0.1237\n",
      "Epoch 66/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9715 - loss: 0.1193\n",
      "Epoch 67/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9760 - loss: 0.1031\n",
      "Epoch 68/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9726 - loss: 0.1180\n",
      "Epoch 69/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9696 - loss: 0.1342\n",
      "Epoch 70/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9767 - loss: 0.0996\n",
      "Epoch 71/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9716 - loss: 0.1157\n",
      "Epoch 72/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9729 - loss: 0.1236\n",
      "Epoch 73/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9769 - loss: 0.1002\n",
      "Epoch 74/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9798 - loss: 0.0944\n",
      "Epoch 75/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9758 - loss: 0.0997\n",
      "Epoch 76/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9708 - loss: 0.1312\n",
      "Epoch 77/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9777 - loss: 0.0967\n",
      "Epoch 78/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9773 - loss: 0.1022\n",
      "Epoch 79/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9770 - loss: 0.0954\n",
      "Epoch 80/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - categorical_accuracy: 0.9716 - loss: 0.1275\n",
      "Epoch 81/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9793 - loss: 0.0917\n",
      "Epoch 82/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9738 - loss: 0.1125\n",
      "Epoch 83/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9795 - loss: 0.0931\n",
      "Epoch 84/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9807 - loss: 0.0861\n",
      "Epoch 85/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9811 - loss: 0.0868\n",
      "Epoch 86/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9717 - loss: 0.1160\n",
      "Epoch 87/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9762 - loss: 0.1001\n",
      "Epoch 88/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9821 - loss: 0.0808\n",
      "Epoch 89/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9818 - loss: 0.0866\n",
      "Epoch 90/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9790 - loss: 0.0973\n",
      "Epoch 91/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9805 - loss: 0.0930\n",
      "Epoch 92/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9830 - loss: 0.0770\n",
      "Epoch 93/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9196 - loss: 0.3099\n",
      "Epoch 94/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9773 - loss: 0.0992\n",
      "Epoch 95/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9803 - loss: 0.0933\n",
      "Epoch 96/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9826 - loss: 0.0786\n",
      "Epoch 97/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9815 - loss: 0.0803\n",
      "Epoch 98/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9846 - loss: 0.0751\n",
      "Epoch 99/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9835 - loss: 0.0766\n",
      "Epoch 100/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9802 - loss: 0.0903\n",
      "Epoch 101/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9799 - loss: 0.0997\n",
      "Epoch 102/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9813 - loss: 0.0830\n",
      "Epoch 103/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9850 - loss: 0.0726\n",
      "Epoch 104/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9838 - loss: 0.0690\n",
      "Epoch 105/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9833 - loss: 0.0735\n",
      "Epoch 106/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9818 - loss: 0.0809\n",
      "Epoch 107/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9846 - loss: 0.0732\n",
      "Epoch 108/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9773 - loss: 0.0964\n",
      "Epoch 109/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9802 - loss: 0.0847\n",
      "Epoch 110/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9821 - loss: 0.0812\n",
      "Epoch 111/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9833 - loss: 0.0765\n",
      "Epoch 112/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9879 - loss: 0.0581\n",
      "Epoch 113/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9857 - loss: 0.0731\n",
      "Epoch 114/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9783 - loss: 0.0923\n",
      "Epoch 115/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9873 - loss: 0.0597\n",
      "Epoch 116/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9846 - loss: 0.0732\n",
      "Epoch 117/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9857 - loss: 0.0643\n",
      "Epoch 118/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9796 - loss: 0.0931\n",
      "Epoch 119/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9885 - loss: 0.0604\n",
      "Epoch 120/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9835 - loss: 0.0776\n",
      "Epoch 121/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9862 - loss: 0.0646\n",
      "Epoch 122/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9869 - loss: 0.0613\n",
      "Epoch 123/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9843 - loss: 0.0761\n",
      "Epoch 124/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9872 - loss: 0.0647\n",
      "Epoch 125/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9831 - loss: 0.0782\n",
      "Epoch 126/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9882 - loss: 0.0576\n",
      "Epoch 127/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9858 - loss: 0.0682\n",
      "Epoch 128/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - categorical_accuracy: 0.9840 - loss: 0.0699\n",
      "Epoch 129/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - categorical_accuracy: 0.9878 - loss: 0.0602\n",
      "Epoch 130/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9850 - loss: 0.0679\n",
      "Epoch 131/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9883 - loss: 0.0551\n",
      "Epoch 132/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9832 - loss: 0.0760\n",
      "Epoch 133/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9844 - loss: 0.0777\n",
      "Epoch 134/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9784 - loss: 0.0981\n",
      "Epoch 135/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9869 - loss: 0.0636\n",
      "Epoch 136/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9890 - loss: 0.0550\n",
      "Epoch 137/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9876 - loss: 0.0618\n",
      "Epoch 138/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9896 - loss: 0.0527\n",
      "Epoch 139/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9869 - loss: 0.0646\n",
      "Epoch 140/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9875 - loss: 0.0575\n",
      "Epoch 141/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9894 - loss: 0.0526\n",
      "Epoch 142/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9840 - loss: 0.0755\n",
      "Epoch 143/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9867 - loss: 0.0600\n",
      "Epoch 144/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9886 - loss: 0.0545\n",
      "Epoch 145/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - categorical_accuracy: 0.9861 - loss: 0.0689\n",
      "Epoch 146/700\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - categorical_accuracy: 0.9915 - loss: 0.0437\n",
      "Reached target accuracy, stopping training...\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - categorical_accuracy: 0.9907 - loss: 0.0449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,300</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m38,144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m3,300\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">587,726</span> (2.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m587,726\u001b[0m (2.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">195,908</span> (765.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m195,908\u001b[0m (765.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">391,818</span> (1.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m391,818\u001b[0m (1.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(15,84)))\n",
    "model.add(Dropout(0.4)) \n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# model.add(GRU(64, return_sequences=True, activation='relu', input_shape=(15,84)))\n",
    "# model.add(Dropout(0.4)) \n",
    "# model.add(GRU(128, return_sequences=True, activation='relu'))\n",
    "# model.add(Dropout(0.2)) \n",
    "# model.add(GRU(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))) \n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001))) \n",
    "model.add(Dense(temp_labels.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "stop_on_acc = StopOnAccuracy(0.99)  # Stop if validation accuracy reaches 99%\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     epochs=2000,\n",
    "#                     callbacks=[stop_on_acc])\n",
    "model.fit(X_train, y_train, epochs=700 ,callbacks=[stop_on_acc])\n",
    "model.summary()\n",
    "\n",
    "# Create a graphical representation of the model architecture\n",
    "keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fdf5806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('action3.h5')\n",
    "model.load_weights('action3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5027fc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Path to your saved model file (.h5 format)\n",
    "\n",
    "# Load the model\n",
    "# modelGRU = load_model(\"action2.h5\")\n",
    "modelLSTM =  load_model(\"action3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2512eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[5180,    0],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5176,    2],\n",
       "        [   0,   42]],\n",
       "\n",
       "       [[5175,    0],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5176,    1],\n",
       "        [   0,   43]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   1,   59]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5148,    0],\n",
       "        [   0,   72]],\n",
       "\n",
       "       [[5175,    0],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5172,    0],\n",
       "        [   0,   48]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5153,    0],\n",
       "        [   0,   67]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5165,    0],\n",
       "        [   1,   54]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   1,   48]],\n",
       "\n",
       "       [[5165,    0],\n",
       "        [   1,   54]],\n",
       "\n",
       "       [[5173,    1],\n",
       "        [   1,   45]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   0,   61]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5184,    0],\n",
       "        [   0,   36]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5161,    1],\n",
       "        [   0,   58]],\n",
       "\n",
       "       [[5168,    0],\n",
       "        [   0,   52]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5181,    0],\n",
       "        [   0,   39]],\n",
       "\n",
       "       [[5166,    0],\n",
       "        [   0,   54]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5159,    1],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5143,    1],\n",
       "        [   1,   75]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5178,    0],\n",
       "        [   0,   42]],\n",
       "\n",
       "       [[5168,    0],\n",
       "        [   1,   51]],\n",
       "\n",
       "       [[5167,    0],\n",
       "        [   0,   53]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5153,    0],\n",
       "        [   0,   67]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   1,   59]],\n",
       "\n",
       "       [[5180,    0],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   0,   61]],\n",
       "\n",
       "       [[5155,    0],\n",
       "        [   0,   65]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5158,    0],\n",
       "        [   0,   62]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5180,    0],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5154,    0],\n",
       "        [   0,   66]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5186,    0],\n",
       "        [   4,   30]],\n",
       "\n",
       "       [[5188,    0],\n",
       "        [   0,   32]],\n",
       "\n",
       "       [[5165,    1],\n",
       "        [   0,   54]],\n",
       "\n",
       "       [[5173,    0],\n",
       "        [   0,   47]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   0,   61]],\n",
       "\n",
       "       [[5172,    0],\n",
       "        [   1,   47]],\n",
       "\n",
       "       [[5144,    0],\n",
       "        [   0,   76]],\n",
       "\n",
       "       [[5172,    0],\n",
       "        [   0,   48]],\n",
       "\n",
       "       [[5152,    0],\n",
       "        [   0,   68]],\n",
       "\n",
       "       [[5177,    0],\n",
       "        [   1,   42]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5172,    0],\n",
       "        [   0,   48]],\n",
       "\n",
       "       [[5166,    0],\n",
       "        [   0,   54]],\n",
       "\n",
       "       [[5167,    0],\n",
       "        [   0,   53]],\n",
       "\n",
       "       [[5175,    0],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   0,   61]],\n",
       "\n",
       "       [[5164,    0],\n",
       "        [   0,   56]],\n",
       "\n",
       "       [[5177,    1],\n",
       "        [   0,   42]],\n",
       "\n",
       "       [[5171,    4],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5176,    1],\n",
       "        [   0,   43]],\n",
       "\n",
       "       [[5183,    0],\n",
       "        [   0,   37]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5168,    0],\n",
       "        [   0,   52]],\n",
       "\n",
       "       [[5180,    0],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5150,    0],\n",
       "        [   0,   70]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5177,    0],\n",
       "        [   0,   43]],\n",
       "\n",
       "       [[5175,    0],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5168,    1],\n",
       "        [   0,   51]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5165,    0],\n",
       "        [   0,   55]],\n",
       "\n",
       "       [[5147,    0],\n",
       "        [   0,   73]],\n",
       "\n",
       "       [[5178,    0],\n",
       "        [   0,   42]],\n",
       "\n",
       "       [[5173,    0],\n",
       "        [   0,   47]],\n",
       "\n",
       "       [[5154,    0],\n",
       "        [   0,   66]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5161,    0],\n",
       "        [   0,   59]],\n",
       "\n",
       "       [[5181,    0],\n",
       "        [   0,   39]],\n",
       "\n",
       "       [[5161,    0],\n",
       "        [   0,   59]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5166,    0],\n",
       "        [   1,   53]],\n",
       "\n",
       "       [[5152,    0],\n",
       "        [   0,   68]],\n",
       "\n",
       "       [[5180,    0],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5162,    0],\n",
       "        [   0,   58]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   0,   61]]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# yhat = modelGRU.predict(X_test)\n",
    "# ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "# yhat = np.argmax(yhat, axis=1).tolist()\n",
    "# multilabel_confusion_matrix(ytrue, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e323d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.9967551234800647\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import recall_score\n",
    "\n",
    "# # Assuming y_true and y_pred are your true and predicted labels respectively\n",
    "# # Calculate recall\n",
    "# recall = recall_score(ytrue, yhat, average='macro')  # Change 'macro' to 'micro' or 'weighted' if needed\n",
    "# print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9975ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9969855023432665\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import precision_score\n",
    "\n",
    "# # Assuming y_true and y_pred are your true and predicted labels respectively\n",
    "# # Calculate precision\n",
    "# precision = precision_score(ytrue, yhat, average='macro')  # Change 'macro' to 'micro' or 'weighted' if needed\n",
    "# print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32f116cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.996795360577872\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # Assuming y_true and y_pred are your true and predicted labels respectively\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(ytrue, yhat, average='macro')  # Change 'macro' to 'micro' or 'weighted' if needed\n",
    "# print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cc83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9971264367816092\n"
     ]
    }
   ],
   "source": [
    "# GRU_ACC= accuracy_score(ytrue, yhat)\n",
    "# print(\"Accuracy\", GRU_ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38ff8f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[5180,    0],\n",
       "        [   1,   39]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5176,    3],\n",
       "        [   1,   40]],\n",
       "\n",
       "       [[5178,    0],\n",
       "        [   0,   42]],\n",
       "\n",
       "       [[5175,    0],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5177,    0],\n",
       "        [   1,   42]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   2,   58]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5146,    2],\n",
       "        [   0,   72]],\n",
       "\n",
       "       [[5170,    5],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5171,    1],\n",
       "        [   0,   48]],\n",
       "\n",
       "       [[5159,    1],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5153,    0],\n",
       "        [   0,   67]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5165,    0],\n",
       "        [   0,   55]],\n",
       "\n",
       "       [[5169,    2],\n",
       "        [   1,   48]],\n",
       "\n",
       "       [[5165,    0],\n",
       "        [   1,   54]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   1,   45]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   1,   45]],\n",
       "\n",
       "       [[5158,    1],\n",
       "        [   1,   60]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5184,    0],\n",
       "        [   0,   36]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5162,    0],\n",
       "        [   0,   58]],\n",
       "\n",
       "       [[5167,    1],\n",
       "        [   1,   51]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5180,    1],\n",
       "        [   0,   39]],\n",
       "\n",
       "       [[5166,    0],\n",
       "        [   1,   53]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5159,    1],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5140,    4],\n",
       "        [   8,   68]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5178,    0],\n",
       "        [   0,   42]],\n",
       "\n",
       "       [[5162,    6],\n",
       "        [   1,   51]],\n",
       "\n",
       "       [[5167,    0],\n",
       "        [   0,   53]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   2,   48]],\n",
       "\n",
       "       [[5150,    3],\n",
       "        [   0,   67]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   2,   58]],\n",
       "\n",
       "       [[5179,    1],\n",
       "        [   1,   39]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   1,   60]],\n",
       "\n",
       "       [[5155,    0],\n",
       "        [   0,   65]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5158,    0],\n",
       "        [   0,   62]],\n",
       "\n",
       "       [[5160,    0],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5171,    0],\n",
       "        [   0,   49]],\n",
       "\n",
       "       [[5159,    1],\n",
       "        [   2,   58]],\n",
       "\n",
       "       [[5180,    0],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5154,    0],\n",
       "        [   2,   64]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5179,    7],\n",
       "        [   0,   34]],\n",
       "\n",
       "       [[5188,    0],\n",
       "        [   0,   32]],\n",
       "\n",
       "       [[5166,    0],\n",
       "        [   0,   54]],\n",
       "\n",
       "       [[5173,    0],\n",
       "        [   0,   47]],\n",
       "\n",
       "       [[5158,    1],\n",
       "        [   2,   59]],\n",
       "\n",
       "       [[5170,    2],\n",
       "        [   2,   46]],\n",
       "\n",
       "       [[5144,    0],\n",
       "        [   1,   75]],\n",
       "\n",
       "       [[5172,    0],\n",
       "        [   1,   47]],\n",
       "\n",
       "       [[5152,    0],\n",
       "        [   0,   68]],\n",
       "\n",
       "       [[5174,    3],\n",
       "        [   0,   43]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5172,    0],\n",
       "        [   1,   47]],\n",
       "\n",
       "       [[5166,    0],\n",
       "        [   0,   54]],\n",
       "\n",
       "       [[5166,    1],\n",
       "        [   0,   53]],\n",
       "\n",
       "       [[5175,    0],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   1,   60]],\n",
       "\n",
       "       [[5164,    0],\n",
       "        [   1,   55]],\n",
       "\n",
       "       [[5178,    0],\n",
       "        [   3,   39]],\n",
       "\n",
       "       [[5173,    2],\n",
       "        [   7,   38]],\n",
       "\n",
       "       [[5177,    0],\n",
       "        [   0,   43]],\n",
       "\n",
       "       [[5183,    0],\n",
       "        [   0,   37]],\n",
       "\n",
       "       [[5169,    2],\n",
       "        [   2,   47]],\n",
       "\n",
       "       [[5179,    0],\n",
       "        [   0,   41]],\n",
       "\n",
       "       [[5159,    1],\n",
       "        [   0,   60]],\n",
       "\n",
       "       [[5168,    0],\n",
       "        [   0,   52]],\n",
       "\n",
       "       [[5180,    0],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5149,    1],\n",
       "        [   0,   70]],\n",
       "\n",
       "       [[5172,    2],\n",
       "        [   2,   44]],\n",
       "\n",
       "       [[5177,    0],\n",
       "        [   1,   42]],\n",
       "\n",
       "       [[5175,    0],\n",
       "        [   0,   45]],\n",
       "\n",
       "       [[5169,    0],\n",
       "        [   0,   51]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   3,   43]],\n",
       "\n",
       "       [[5165,    0],\n",
       "        [   2,   53]],\n",
       "\n",
       "       [[5147,    0],\n",
       "        [   0,   73]],\n",
       "\n",
       "       [[5178,    0],\n",
       "        [   0,   42]],\n",
       "\n",
       "       [[5172,    1],\n",
       "        [   1,   46]],\n",
       "\n",
       "       [[5154,    0],\n",
       "        [   1,   65]],\n",
       "\n",
       "       [[5174,    0],\n",
       "        [   0,   46]],\n",
       "\n",
       "       [[5170,    0],\n",
       "        [   0,   50]],\n",
       "\n",
       "       [[5159,    2],\n",
       "        [   0,   59]],\n",
       "\n",
       "       [[5181,    0],\n",
       "        [   0,   39]],\n",
       "\n",
       "       [[5161,    0],\n",
       "        [   0,   59]],\n",
       "\n",
       "       [[5151,    0],\n",
       "        [   0,   69]],\n",
       "\n",
       "       [[5165,    1],\n",
       "        [   0,   54]],\n",
       "\n",
       "       [[5152,    0],\n",
       "        [   2,   66]],\n",
       "\n",
       "       [[5179,    1],\n",
       "        [   0,   40]],\n",
       "\n",
       "       [[5158,    4],\n",
       "        [   0,   58]],\n",
       "\n",
       "       [[5159,    0],\n",
       "        [   0,   61]]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "yhat = modelLSTM.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46f074f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.9878946843899962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Assuming y_true and y_pred are your true and predicted labels respectively\n",
    "# Calculate recall\n",
    "recall = recall_score(ytrue, yhat, average='macro')  # Change 'macro' to 'micro' or 'weighted' if needed\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a39559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9876503610225201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Assuming y_true and y_pred are your true and predicted labels respectively\n",
    "# Calculate precision\n",
    "precision = precision_score(ytrue, yhat, average='macro')  # Change 'macro' to 'micro' or 'weighted' if needed\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "047ad456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9875062835804083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are your true and predicted labels respectively\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(ytrue, yhat, average='macro')  # Change 'macro' to 'micro' or 'weighted' if needed\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c467fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9877394636015325\n"
     ]
    }
   ],
   "source": [
    "LSTM_ACC= accuracy_score(ytrue, yhat)\n",
    "print(\"Accuracy\", LSTM_ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa839a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31df29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign_language_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
